{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Post Classification Model Evaluation\n",
    "# ==========================================\n",
    "# This notebook evaluates the performance of our fine-tuned Llama 4 Scout model \n",
    "# on the task of classifying Reddit posts into six categories:\n",
    "# Entertainment, Health, Comedy, Profession, Travel, and Education\n",
    "\n",
    "# Make sure you are using a Python 3.13.13 .venv environment\n",
    "\n",
    "# Install required packages\n",
    "# Note: sklearn is imported as a module, but the package name for installation is scikit-learn\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn torch transformers bert-score rouge-score nltk tqdm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATEGORIES = [\"Entertainment\", \"Health\", \"Comedy\", \"Profession\", \"Travel\", \"Education\"]\n",
    "MODEL_PATH = \"path/to/finetuned/model\"              # Update with MODEL PATH\n",
    "BASELINE_MODEL_PATH = \"meta-llama/Llama-4-Scout\"    # For zero-shot comparison\n",
    "TEST_DATA_PATH = \"path/to/test/data.csv\"            # Update with TEST DATA PATH\n",
    "RESULTS_DIR = \"evaluation_results\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Running on device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "# ================================\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the test dataset and preprocess it for evaluation.\n",
    "    \n",
    "    Format expected:\n",
    "    - Each row contains: post title, post body, top 5 comments, subreddit name, and topic label\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verify columns exist\n",
    "    required_columns = ['post_title', 'post_body', 'comments', 'subreddit', 'category']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in dataset\")\n",
    "    \n",
    "    # Ensure comments are properly formatted (as a list of 5 comments)\n",
    "    if isinstance(df['comments'].iloc[0], str):\n",
    "        # If comments are stored as a string, convert to list\n",
    "        df['comments'] = df['comments'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Verify that all categories are valid\n",
    "    invalid_categories = set(df['category']) - set(CATEGORIES)\n",
    "    if invalid_categories:\n",
    "        print(f\"Warning: Found invalid categories in dataset: {invalid_categories}\")\n",
    "        # Filter out invalid categories\n",
    "        df = df[df['category'].isin(CATEGORIES)]\n",
    "    \n",
    "    print(f\"Loaded test dataset with {len(df)} samples\")\n",
    "    print(f\"Category distribution: {df['category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def format_input_for_model(row):\n",
    "    \"\"\"\n",
    "    Format the input data for the model according to our fine-tuning format:\n",
    "    \n",
    "    Post title: <title>\n",
    "    Post body: <body>\n",
    "    Comment 1: <comment>\n",
    "    ...\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    # Start with post title and body\n",
    "    formatted_input = f\"Post title: {row['post_title']}\\nPost body: {row['post_body']}\\n\"\n",
    "    \n",
    "    # Add comments\n",
    "    for i, comment in enumerate(row['comments'][:5]):  # Only include up to 5 comments\n",
    "        formatted_input += f\"Comment {i+1}: {comment}\\n\"\n",
    "    \n",
    "    # Add the category prompt\n",
    "    formatted_input += \"Category:\"\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "# Load test data\n",
    "try:\n",
    "    test_df = load_test_data(TEST_DATA_PATH)\n",
    "    print(\"Successfully loaded test data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "    # Create a sample test dataset for testing\n",
    "    print(\"Creating sample test dataset...\")\n",
    "    \n",
    "    #=========================\n",
    "    # EXAMPLE DATA FOR TESTING\n",
    "    #=========================\n",
    "    sample_data = []\n",
    "    for _ in range(50):  # 50 sample entries\n",
    "        category = random.choice(CATEGORIES)\n",
    "        subreddit = f\"r/{category.lower()}\"\n",
    "        sample_data.append({\n",
    "            'post_title': f\"Sample post about {category}\",\n",
    "            'post_body': f\"This is a sample post body about {category}\",\n",
    "            'comments': [f\"Comment {i+1} about {category}\" for i in range(5)],\n",
    "            'subreddit': subreddit,\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "    print(\"Created sample test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Loading\n",
    "# ===============\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load the fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "ft_model, ft_tokenizer = load_model(MODEL_PATH)\n",
    "if ft_model is None:\n",
    "    print(\"Failed to load fine-tuned model. Check the model path.\")\n",
    "    # For demonstration, we'll create placeholder prediction functions\n",
    "    def predict_category_ft(*args, **kwargs):\n",
    "        return random.choice(CATEGORIES)\n",
    "else:\n",
    "    print(\"Successfully loaded fine-tuned model\")\n",
    "    \n",
    "    def predict_category_ft(input_text, model=ft_model, tokenizer=ft_tokenizer):\n",
    "        \"\"\"Generate category prediction using the fine-tuned model\"\"\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # We only need a short output for the category\n",
    "                temperature=0.1,    # Lower temperature for more deterministic outputs\n",
    "                do_sample=False,    # We want deterministic outputs for evaluation\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the output and extract the category\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # The output might contain the input text plus the generated category\n",
    "        # Extract just the category name from the generated output\n",
    "        for category in CATEGORIES:\n",
    "            if category in generated_text[len(input_text):]:\n",
    "                return category\n",
    "        \n",
    "        # If no exact category match, use some heuristics to find the closest one\n",
    "        generated_category = generated_text[len(input_text):].strip()\n",
    "        \n",
    "        # Simple approach: return the category with highest overlap with generated text\n",
    "        max_overlap = 0\n",
    "        best_category = CATEGORIES[0]  # Default to first category\n",
    "        \n",
    "        for category in CATEGORIES:\n",
    "            # Calculate word overlap\n",
    "            overlap = len(set(word_tokenize(generated_category.lower())) & \n",
    "                          set(word_tokenize(category.lower())))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_category = category\n",
    "        \n",
    "        return best_category\n",
    "\n",
    "# Load the baseline model for zero-shot comparison\n",
    "print(\"Loading baseline model for zero-shot evaluation...\")\n",
    "baseline_model, baseline_tokenizer = load_model(BASELINE_MODEL_PATH)\n",
    "if baseline_model is None:\n",
    "    print(\"Failed to load baseline model. Zero-shot comparison will be skipped.\")\n",
    "    # Placeholder prediction function for baseline\n",
    "    def predict_category_baseline(*args, **kwargs):\n",
    "        return random.choice(CATEGORIES)\n",
    "else:\n",
    "    print(\"Successfully loaded baseline model\")\n",
    "    \n",
    "    def predict_category_baseline(input_text, model=baseline_model, tokenizer=baseline_tokenizer):\n",
    "        \"\"\"Generate zero-shot category prediction using the baseline model\"\"\"\n",
    "        # For zero-shot, we need to modify the prompt to include all possible categories\n",
    "        zero_shot_prompt = input_text + \" (Choose one category from: Entertainment, Health, Comedy, Profession, Travel, Education)\\nCategory:\"\n",
    "        \n",
    "        inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the category from the generated output\n",
    "        for category in CATEGORIES:\n",
    "            if category in generated_text[len(zero_shot_prompt):]:\n",
    "                return category\n",
    "        \n",
    "        # If no exact match, use heuristics as before\n",
    "        generated_category = generated_text[len(zero_shot_prompt):].strip()\n",
    "        \n",
    "        # Return the category with highest overlap\n",
    "        max_overlap = 0\n",
    "        best_category = CATEGORIES[0]\n",
    "        \n",
    "        for category in CATEGORIES:\n",
    "            overlap = len(set(word_tokenize(generated_category.lower())) & \n",
    "                         set(word_tokenize(category.lower())))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_category = category\n",
    "        \n",
    "        return best_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluation Functions\n",
    "# =====================\n",
    "\n",
    "def calculate_basic_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy, precision, recall, and F1 scores\"\"\"\n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Per-class precision, recall, and F1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=CATEGORIES, average=None\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Format results as a dictionary\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'class_metrics': {}\n",
    "    }\n",
    "    \n",
    "    for i, category in enumerate(CATEGORIES):\n",
    "        results['class_metrics'][category] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_bert_score(references, predictions):\n",
    "    \"\"\"Calculate BERTScore for semantic similarity\"\"\"\n",
    "    try:\n",
    "        P, R, F1 = bert_score(predictions, references, lang='en', model_type='microsoft/deberta-xlarge-mnli')\n",
    "        # Convert tensor to float\n",
    "        return {\n",
    "            'precision': P.mean().item(),\n",
    "            'recall': R.mean().item(),\n",
    "            'f1': F1.mean().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERTScore: {e}\")\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0\n",
    "        }\n",
    "\n",
    "def calculate_rouge_score(references, predictions):\n",
    "    \"\"\"Calculate ROUGE-L score for text overlap\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "    \n",
    "    for ref, pred in zip(references, predictions):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return {'rouge_l': np.mean(scores)}\n",
    "\n",
    "def create_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Create and plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=CATEGORIES)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Category')\n",
    "    plt.xlabel('Predicted Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def analyze_misclassifications(test_df, predictions):\n",
    "    \"\"\"Analyze patterns in misclassified examples\"\"\"\n",
    "    test_df['predicted_category'] = predictions\n",
    "    test_df['is_correct'] = test_df['category'] == test_df['predicted_category']\n",
    "    \n",
    "    # Identify the most commonly misclassified categories\n",
    "    misclassified = test_df[~test_df['is_correct']]\n",
    "    \n",
    "    if len(misclassified) == 0:\n",
    "        return {\"error_analysis\": \"No misclassifications found\"}\n",
    "    \n",
    "    # Calculate error rates by true category\n",
    "    error_by_category = {}\n",
    "    for category in CATEGORIES:\n",
    "        category_samples = test_df[test_df['category'] == category]\n",
    "        if len(category_samples) > 0:\n",
    "            error_rate = 1 - category_samples['is_correct'].mean()\n",
    "            error_by_category[category] = error_rate\n",
    "    \n",
    "    # Find the most common misclassification pairs\n",
    "    misclass_pairs = Counter()\n",
    "    for _, row in misclassified.iterrows():\n",
    "        pair = (row['category'], row['predicted_category'])\n",
    "        misclass_pairs[pair] += 1\n",
    "    \n",
    "    most_common_errors = misclass_pairs.most_common(5)\n",
    "    \n",
    "    # Sample misclassified examples for qualitative analysis\n",
    "    sample_size = min(10, len(misclassified))\n",
    "    sample_errors = misclassified.sample(sample_size)\n",
    "    \n",
    "    error_analysis = {\n",
    "        \"error_rates_by_category\": error_by_category,\n",
    "        \"most_common_misclassifications\": [\n",
    "            {\n",
    "                \"true_category\": true_cat,\n",
    "                \"predicted_category\": pred_cat,\n",
    "                \"count\": count\n",
    "            }\n",
    "            for (true_cat, pred_cat), count in most_common_errors\n",
    "        ],\n",
    "        \"sample_misclassified\": [\n",
    "            {\n",
    "                \"post_title\": row['post_title'],\n",
    "                \"true_category\": row['category'],\n",
    "                \"predicted_category\": row['predicted_category'],\n",
    "                \"subreddit\": row['subreddit']\n",
    "            }\n",
    "            for _, row in sample_errors.iterrows()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return error_analysis\n",
    "\n",
    "def generate_evaluation_report(basic_metrics, bert_scores, rouge_scores, error_analysis):\n",
    "    \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
    "    report = {\n",
    "        \"basic_metrics\": basic_metrics,\n",
    "        \"semantic_similarity\": {\n",
    "            \"bert_score\": bert_scores,\n",
    "            \"rouge_score\": rouge_scores\n",
    "        },\n",
    "        \"error_analysis\": error_analysis\n",
    "    }\n",
    "    \n",
    "    with open(f\"{RESULTS_DIR}/evaluation_report.json\", 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Create a summary markdown report\n",
    "    markdown_report = f\"\"\"# Reddit Classification Model Evaluation Report\n",
    "    \n",
    "    ## Overview\n",
    "\n",
    "- **Model**: Fine-tuned Llama 4 Scout\n",
    "- **Task**: Classify Reddit posts into 6 categories\n",
    "- **Test Set Size**: {len(test_df)} samples\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Classification Accuracy\n",
    "\n",
    "- **Overall Accuracy**: {basic_metrics['accuracy']:.4f} (Target: ≥0.85)\n",
    "- **Macro F1 Score**: {basic_metrics['macro_f1']:.4f} (Target: ≥0.82)\n",
    "\n",
    "### Per-Category Performance\n",
    "\n",
    "| Category | Precision | Recall | F1 Score | Support |\n",
    "|----------|-----------|--------|----------|---------|\n",
    "\"\"\"\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        metrics = basic_metrics['class_metrics'][category]\n",
    "        markdown_report += f\"| {category} | {metrics['precision']:.4f} | {metrics['recall']:.4f} | {metrics['f1']:.4f} | {metrics['support']} |\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "### Semantic Similarity Metrics\n",
    "\n",
    "- **BERTScore F1**: {bert_scores['f1']:.4f} (Target: ≥0.88)\n",
    "- **ROUGE-L Score**: {rouge_scores['rouge_l']:.4f} (Target: ≥0.75)\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "### Most Challenging Categories\n",
    "\n",
    "The categories with the highest error rates:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Sort categories by error rate (descending)\n",
    "    error_rates = error_analysis.get(\"error_rates_by_category\", {})\n",
    "    sorted_error_rates = sorted(error_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for category, error_rate in sorted_error_rates[:3]:  # Top 3 most challenging\n",
    "        markdown_report += f\"- **{category}**: {error_rate:.4f} error rate\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "### Common Misclassifications\n",
    "\n",
    "The most common category confusions:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for error in error_analysis.get(\"most_common_misclassifications\", [])[:3]:\n",
    "        markdown_report += f\"- **{error['true_category']}** misclassified as **{error['predicted_category']}**: {error['count']} instances\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "## Conclusion\n",
    "\n",
    "The model {'meets' if basic_metrics['accuracy'] >= 0.85 else 'does not meet'} our target accuracy benchmark of ≥0.85.\n",
    "The BERTScore F1 {'meets' if bert_scores['f1'] >= 0.88 else 'does not meet'} our target of ≥0.88.\n",
    "The ROUGE-L score {'meets' if rouge_scores['rouge_l'] >= 0.75 else 'does not meet'} our target of ≥0.75.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{RESULTS_DIR}/evaluation_summary.md\", 'w') as f:\n",
    "        f.write(markdown_report)\n",
    "    \n",
    "    return markdown_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluation Execution\n",
    "# =====================\n",
    "\n",
    "def evaluate_model(model_type=\"fine-tuned\"):\n",
    "    \"\"\"Run full evaluation for the specified model type\"\"\"\n",
    "    print(f\"Starting evaluation of {model_type} model...\")\n",
    "    \n",
    "    # Format inputs\n",
    "    inputs = test_df.apply(format_input_for_model, axis=1).tolist()\n",
    "    true_categories = test_df['category'].tolist()\n",
    "    \n",
    "    # Make predictions based on model type\n",
    "    if model_type == \"fine-tuned\":\n",
    "        predict_fn = predict_category_ft\n",
    "    else:  # baseline\n",
    "        predict_fn = predict_category_baseline\n",
    "    \n",
    "    # Run predictions\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Making predictions with {model_type} model...\")\n",
    "    for input_text in tqdm(inputs):\n",
    "        prediction = predict_fn(input_text)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"Calculating evaluation metrics...\")\n",
    "    basic_metrics = calculate_basic_metrics(true_categories, predictions)\n",
    "    \n",
    "    # For BERTScore and ROUGE, pass the category names as simple text\n",
    "    bert_scores = calculate_bert_score(true_categories, predictions)\n",
    "    rouge_scores = calculate_rouge_score(true_categories, predictions)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    if model_type == \"fine-tuned\":  # Only create plots for fine-tuned model\n",
    "        print(\"Creating confusion matrix...\")\n",
    "        cm = create_confusion_matrix(true_categories, predictions)\n",
    "    \n",
    "    # Analyze misclassifications (only for fine-tuned model)\n",
    "    if model_type == \"fine-tuned\":\n",
    "        print(\"Analyzing misclassifications...\")\n",
    "        error_analysis = analyze_misclassifications(test_df.copy(), predictions)\n",
    "    else:\n",
    "        error_analysis = {}\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        \"model_type\": model_type,\n",
    "        \"basic_metrics\": basic_metrics,\n",
    "        \"bert_scores\": bert_scores,\n",
    "        \"rouge_scores\": rouge_scores,\n",
    "        \"predictions\": predictions\n",
    "    }\n",
    "    \n",
    "    with open(f\"{RESULTS_DIR}/{model_type}_results.json\", 'w') as f:\n",
    "        # Convert numpy values to float for JSON serialization\n",
    "        json_results = {\n",
    "            \"model_type\": results[\"model_type\"],\n",
    "            \"basic_metrics\": results[\"basic_metrics\"],\n",
    "            \"bert_scores\": {k: float(v) for k, v in results[\"bert_scores\"].items()},\n",
    "            \"rouge_scores\": {k: float(v) for k, v in results[\"rouge_scores\"].items()},\n",
    "            \"predictions\": results[\"predictions\"]\n",
    "        }\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation of {model_type} model completed\")\n",
    "    return results, error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compare Fine-tuned vs. Baseline Performance\n",
    "# ===========================================\n",
    "\n",
    "def compare_models(ft_results, baseline_results):\n",
    "    \"\"\"Compare performance between fine-tuned and baseline models\"\"\"\n",
    "    comparison = {\n",
    "        \"accuracy_improvement\": ft_results[\"basic_metrics\"][\"accuracy\"] - baseline_results[\"basic_metrics\"][\"accuracy\"],\n",
    "        \"f1_improvement\": ft_results[\"basic_metrics\"][\"macro_f1\"] - baseline_results[\"basic_metrics\"][\"macro_f1\"],\n",
    "        \"bert_score_improvement\": ft_results[\"bert_scores\"][\"f1\"] - baseline_results[\"bert_scores\"][\"f1\"],\n",
    "        \"rouge_improvement\": ft_results[\"rouge_scores\"][\"rouge_l\"] - baseline_results[\"rouge_scores\"][\"rouge_l\"]\n",
    "    }\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Accuracy comparison by category\n",
    "    metrics_by_category = {\n",
    "        \"Fine-tuned\": {},\n",
    "        \"Baseline\": {}\n",
    "    }\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        metrics_by_category[\"Fine-tuned\"][category] = ft_results[\"basic_metrics\"][\"class_metrics\"][category][\"f1\"]\n",
    "        metrics_by_category[\"Baseline\"][category] = baseline_results[\"basic_metrics\"][\"class_metrics\"][category][\"f1\"]\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    comparison_df = pd.DataFrame(metrics_by_category)\n",
    "    \n",
    "    # Plot F1 scores by category\n",
    "    comparison_df.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title('F1 Score Comparison by Category')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Category')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/f1_comparison_by_category.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot overall metrics comparison\n",
    "    metrics = ['accuracy', 'macro_f1']\n",
    "    ft_values = [ft_results[\"basic_metrics\"][\"accuracy\"], ft_results[\"basic_metrics\"][\"macro_f1\"]]\n",
    "    baseline_values = [baseline_results[\"basic_metrics\"][\"accuracy\"], baseline_results[\"basic_metrics\"][\"macro_f1\"]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, ft_values, width, label='Fine-tuned')\n",
    "    plt.bar(x + width/2, baseline_values, width, label='Baseline')\n",
    "    \n",
    "    plt.xticks(x, ['Accuracy', 'Macro F1'])\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Comparison: Fine-tuned vs. Baseline')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/overall_metrics_comparison.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate comparison report\n",
    "    comparison_report = f\"\"\"# Model Comparison: Fine-tuned vs. Baseline\n",
    "\n",
    "## Overall Improvement\n",
    "\n",
    "- **Accuracy Improvement**: {comparison[\"accuracy_improvement\"]:.4f} \n",
    "  - Fine-tuned: {ft_results[\"basic_metrics\"][\"accuracy\"]:.4f}\n",
    "  - Baseline: {baseline_results[\"basic_metrics\"][\"accuracy\"]:.4f}\n",
    "\n",
    "- **Macro F1 Improvement**: {comparison[\"f1_improvement\"]:.4f}\n",
    "  - Fine-tuned: {ft_results[\"basic_metrics\"][\"macro_f1\"]:.4f}\n",
    "  - Baseline: {baseline_results[\"basic_metrics\"][\"macro_f1\"]:.4f}\n",
    "\n",
    "- **BERTScore F1 Improvement**: {comparison[\"bert_score_improvement\"]:.4f}\n",
    "  - Fine-tuned: {ft_results[\"bert_scores\"][\"f1\"]:.4f}\n",
    "  - Baseline: {baseline_results[\"bert_scores\"][\"f1\"]:.4f}\n",
    "\n",
    "- **ROUGE-L Improvement**: {comparison[\"rouge_improvement\"]:.4f}\n",
    "  - Fine-tuned: {ft_results[\"rouge_scores\"][\"rouge_l\"]:.4f}\n",
    "  - Baseline: {baseline_results[\"rouge_scores\"][\"rouge_l\"]:.4f}\n",
    "\n",
    "## Per-Category F1 Score Comparison\n",
    "\n",
    "| Category | Fine-tuned | Baseline | Improvement |\n",
    "|----------|------------|----------|-------------|\n",
    "\"\"\"\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        ft_f1 = ft_results[\"basic_metrics\"][\"class_metrics\"][category][\"f1\"]\n",
    "        base_f1 = baseline_results[\"basic_metrics\"][\"class_metrics\"][category][\"f1\"]\n",
    "        improvement = ft_f1 - base_f1\n",
    "        comparison_report += f\"| {category} | {ft_f1:.4f} | {base_f1:.4f} | {improvement:.4f} |\\n\"\n",
    "    \n",
    "    with open(f\"{RESULTS_DIR}/model_comparison.md\", 'w') as f:\n",
    "        f.write(comparison_report)\n",
    "    \n",
    "    return comparison, comparison_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting model evaluation...\")\n",
    "    \n",
    "    # Evaluate fine-tuned model\n",
    "    ft_results, error_analysis = evaluate_model(\"fine-tuned\")\n",
    "    \n",
    "    # Evaluate baseline model\n",
    "    baseline_results, _ = evaluate_model(\"baseline\")\n",
    "    \n",
    "    # Compare models\n",
    "    comparison, comparison_report = compare_models(ft_results, baseline_results)\n",
    "    \n",
    "    # Generate full report for fine-tuned model\n",
    "    report = generate_evaluation_report(\n",
    "        ft_results[\"basic_metrics\"],\n",
    "        ft_results[\"bert_scores\"],\n",
    "        ft_results[\"rouge_scores\"],\n",
    "        error_analysis\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Fine-tuned model accuracy: {ft_results['basic_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Baseline model accuracy: {baseline_results['basic_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Improvement: {comparison['accuracy_improvement']:.4f}\")\n",
    "    print(f\"\\nFine-tuned model F1: {ft_results['basic_metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"Baseline model F1: {baseline_results['basic_metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"Improvement: {comparison['f1_improvement']:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed reports saved to:\")\n",
    "    print(f\"- {RESULTS_DIR}/evaluation_report.json\")\n",
    "    print(f\"- {RESULTS_DIR}/evaluation_summary.md\")\n",
    "    print(f\"- {RESULTS_DIR}/model_comparison.md\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
