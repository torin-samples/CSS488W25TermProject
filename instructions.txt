Key Features of the Evaluation Notebook

Comprehensive Metrics Implementation:

Standard classification metrics (accuracy, precision, recall, F1 score)
Macro-F1 for class-imbalance robustness
BERTScore for semantic similarity between predicted and true labels
ROUGE-L for sequence alignment
Confusion matrix visualization


Comparative Analysis:

Zero-shot baseline comparison with the unmodified Llama 4 Scout
Per-category performance comparison
Visual representations of model improvements


In-depth Error Analysis:

Identification of most challenging categories
Analysis of common misclassification patterns
Qualitative assessment of sample misclassified instances


Detailed Reporting:

JSON reports with all evaluation metrics
Markdown summaries with key findings
Visualizations saved for easy inclusion in presentations



How to Use the Notebook
Before running the notebook, you'll need to update a few variables:

MODEL_PATH: Path to your fine-tuned model
TEST_DATA_PATH: Path to your test dataset
You might also need to adjust the data loading function based on your specific CSV format