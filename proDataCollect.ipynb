{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b59b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "#Use Flask to handle incoming requests from redirect URI from Reddit\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/reddit_callback')\n",
    "def reddit_callback():\n",
    "    # Retrieve the authorization code or access token from the URL parameters\n",
    "    authorization_code = request.args.get('code')\n",
    "    # Do something with the authorization code, such as exchanging it for an access token\n",
    "    # Or, store it for later use\n",
    "    return \"Callback received successfully\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b804c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from r/CareerAdvice [hot]...\n",
      "Collecting from r/CareerAdvice [new]...\n",
      "Error collecting from r/CareerAdvice: received 429 HTTP response\n",
      "Collecting from r/jobs [hot]...\n",
      "Collecting from r/jobs [new]...\n",
      "Collecting from r/jobs [top]...\n",
      "Collecting from r/jobs [controversial]...\n",
      "Collecting from r/resumes [hot]...\n",
      "Error collecting from r/resumes: received 429 HTTP response\n",
      "Collecting from r/careerguidance [hot]...\n",
      "Collecting from r/careerguidance [new]...\n",
      "Error collecting from r/careerguidance: received 429 HTTP response\n",
      "Collecting from r/cscareerquestions [hot]...\n",
      "Collecting from r/cscareerquestions [new]...\n",
      "Error collecting from r/cscareerquestions: received 429 HTTP response\n",
      "Collecting from r/AskHR [hot]...\n",
      "Collecting from r/AskHR [new]...\n",
      "Error collecting from r/AskHR: received 429 HTTP response\n",
      "Collecting from r/recruitinghell [hot]...\n",
      "Collecting from r/recruitinghell [new]...\n",
      "Error collecting from r/recruitinghell: received 429 HTTP response\n",
      "Collecting from r/LinkedInLunatics [hot]...\n",
      "Collecting from r/LinkedInLunatics [new]...\n",
      "Error collecting from r/LinkedInLunatics: received 429 HTTP response\n",
      "Collecting from r/interviews [hot]...\n",
      "Collecting from r/interviews [new]...\n",
      "Error collecting from r/interviews: received 429 HTTP response\n",
      "Collecting from r/work [hot]...\n",
      "Collecting from r/work [new]...\n",
      "Error collecting from r/work: received 429 HTTP response\n",
      "\n",
      "Total posts collected: 11902\n",
      "Final dataset shape: (45012, 6)\n",
      "Breakdown by type: type\n",
      "post         10180\n",
      "comment_1     8611\n",
      "comment_2     7633\n",
      "comment_3     6756\n",
      "comment_4     6182\n",
      "comment_5     5650\n",
      "Name: count, dtype: int64\n",
      "Breakdown by subreddit: subreddit\n",
      "r/jobs                 14553\n",
      "r/LinkedInLunatics      4934\n",
      "r/cscareerquestions     4208\n",
      "r/work                  4070\n",
      "r/recruitinghell        3911\n",
      "r/interviews            3593\n",
      "r/CareerAdvice          3061\n",
      "r/AskHR                 2772\n",
      "r/careerguidance        2504\n",
      "r/resumes               1406\n",
      "Name: count, dtype: int64\n",
      "Dataset saved as 'professional_dataset.csv'\n",
      "Original format saved as 'Reddit_professional_original.pkl'\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "    user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "    check_for_async=False\n",
    ")\n",
    "\n",
    "# List of professional/career subreddits\n",
    "professional_subreddits = [\n",
    "    'CareerAdvice',\n",
    "    'jobs',\n",
    "    'resumes',\n",
    "    'careerguidance',\n",
    "    'cscareerquestions',\n",
    "    'AskHR',\n",
    "    'recruitinghell',\n",
    "    'LinkedInLunatics',\n",
    "    'interviews',\n",
    "    'work'\n",
    "]\n",
    "\n",
    "def is_moderator_or_bot_content(text, author_name=None):\n",
    "    \"\"\"Filter out bot/moderator content.\"\"\"\n",
    "    bot_usernames = [\n",
    "        'AutoModerator', 'moderator', 'mod', 'bot', 'WikiTextBot', 'RepostSleuthBot',\n",
    "        'RemindMeBot', 'TweetPoster'\n",
    "    ]\n",
    "    if author_name:\n",
    "        author_lower = author_name.lower()\n",
    "        for bot_name in bot_usernames:\n",
    "            if bot_name.lower() in author_lower:\n",
    "                return True\n",
    "    mod_bot_phrases = [\n",
    "        r'this is a friendly reminder',\n",
    "        r'your post has been removed',\n",
    "        r'this comment has been removed',\n",
    "        r'are not allowed',\n",
    "        r'please read the rules',\n",
    "        r'violates rule',\n",
    "        r'breaking rule',\n",
    "        r'temporary ban',\n",
    "        r'permanently banned',\n",
    "        r'moderator action',\n",
    "        r'mod note',\n",
    "        r'subreddit rules',\n",
    "        r'community guidelines',\n",
    "        r'please contact the moderators',\n",
    "        r'message the mods',\n",
    "        r'if you have questions',\n",
    "        r'appeal this action',\n",
    "        r'repost will be removed',\n",
    "        r'spam filter',\n",
    "        r'automatically removed',\n",
    "        r'bot response',\n",
    "        r'i am a bot',\n",
    "        r'beep boop',\n",
    "        r'this action was performed automatically',\n",
    "        r'if you believe this was done in error',\n",
    "        r'contact.*moderator',\n",
    "        r'your submission.*removed',\n",
    "        r'thank you for your submission',\n",
    "        r'please ensure',\n",
    "        r'reminder.*rule',\n",
    "        r'this post.*locked',\n",
    "        r'comments.*locked'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for phrase in mod_bot_phrases:\n",
    "        if re.search(phrase, text_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "sorts = ['hot', 'new', 'top', 'controversial']\n",
    "\n",
    "def collect_subreddit_data(subreddit_name, limit=1000, comment_limit=20):\n",
    "    \"\"\"Collect posts and top N comments from a specific subreddit using multiple sorts.\"\"\"\n",
    "    posts_data = []\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for sort in sorts:\n",
    "            print(f\"Collecting from r/{subreddit_name} [{sort}]...\")\n",
    "            submissions = getattr(subreddit, sort)(limit=limit)\n",
    "            for submission in submissions:\n",
    "                if submission.stickied:\n",
    "                    continue\n",
    "                if is_moderator_or_bot_content(submission.title + \" \" + submission.selftext, submission.author.name if submission.author else None):\n",
    "                    continue\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                comments = submission.comments.list()\n",
    "                filtered_comments = []\n",
    "                for comment in comments:\n",
    "                    if hasattr(comment, 'body') and hasattr(comment, 'author'):\n",
    "                        author_name = comment.author.name if comment.author else None\n",
    "                        if not is_moderator_or_bot_content(comment.body, author_name):\n",
    "                            filtered_comments.append(comment)\n",
    "                # Get top N comments by score\n",
    "                top_comments = sorted(filtered_comments, key=lambda x: x.score, reverse=True)[:comment_limit]\n",
    "                comment_texts = [comment.body for comment in top_comments]\n",
    "                while len(comment_texts) < comment_limit:\n",
    "                    comment_texts.append(\"\")\n",
    "                post_data = {\n",
    "                    'post_title': submission.title,\n",
    "                    'post_body': submission.selftext,\n",
    "                    'url': submission.url,\n",
    "                    'top_5_comments': comment_texts[:5],  # keep for compatibility\n",
    "                    'subreddit': f\"r/{subreddit_name}\",\n",
    "                    'category': 'Professional',\n",
    "                    'score': submission.score,\n",
    "                    'num_comments': submission.num_comments\n",
    "                }\n",
    "                posts_data.append(post_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting from r/{subreddit_name}: {e}\")\n",
    "    return posts_data\n",
    "\n",
    "# Collect data from all professional subreddits\n",
    "all_posts = []\n",
    "for subreddit_name in professional_subreddits:\n",
    "    all_posts.extend(collect_subreddit_data(subreddit_name, limit=1000, comment_limit=20))\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nTotal posts collected: {len(all_posts)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Expand posts and comments into individual rows (as before)\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    post_row = {\n",
    "        'text': f\"{row['post_title']} {row['post_body']}\".strip(),\n",
    "        'type': 'post',\n",
    "        'subreddit': row['subreddit'],\n",
    "        'category': row['category'],\n",
    "        'score': row['score'],\n",
    "        'url': row['url']\n",
    "    }\n",
    "    expanded_rows.append(post_row)\n",
    "    for i, comment in enumerate(row['top_5_comments']):\n",
    "        if comment.strip():\n",
    "            comment_row = {\n",
    "                'text': comment,\n",
    "                'type': f'comment_{i+1}',\n",
    "                'subreddit': row['subreddit'],\n",
    "                'category': row['category'],\n",
    "                'score': None,\n",
    "                'url': row['url']\n",
    "            }\n",
    "            expanded_rows.append(comment_row)\n",
    "\n",
    "final_df = pd.DataFrame(expanded_rows)\n",
    "final_df.drop_duplicates(subset=['text', 'type', 'subreddit', 'url'], inplace=True)\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Breakdown by type: {final_df['type'].value_counts()}\")\n",
    "print(f\"Breakdown by subreddit: {final_df['subreddit'].value_counts()}\")\n",
    "\n",
    "# Save the dataset (same format as before)\n",
    "final_df.to_csv('larger_professional_dataset.csv', index=False)\n",
    "print(\"Dataset saved as 'professional_dataset.csv'\")\n",
    "\n",
    "df.to_pickle('larger_Reddit_professional_original.pkl')\n",
    "print(\"Original format saved as 'Reddit_professional_original.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
