{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Jupiter, it's ok to start running this cell and stop an execution in a couple of seconds\n",
    "#!pip install praw\n",
    "#!pip install flask\n",
    "\n",
    "#Use Flask to handle incoming requests from redirect URI from Reddit\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/reddit_callback')\n",
    "def reddit_callback():\n",
    "    # Retrieve the authorization code or access token from the URL parameters\n",
    "    authorization_code = request.args.get('code')\n",
    "    # Do something with the authorization code, such as exchanging it for an access token\n",
    "    # Or, store it for later use\n",
    "    return \"Callback received successfully\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather data from multiple subreddits using PRAW\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Initialize PRAW with your Reddit API credentials\n",
    "# SET BEFORE RUNNING\n",
    "reddit = praw.Reddit(\n",
    "    client_id='YOUR_CLIENT_ID',\n",
    "    client_secret='YOUR_CLIENT_SECRET',\n",
    "    user_agent='YOUR_USER_AGENT',\n",
    "    check_for_async=False\n",
    ")\n",
    "\n",
    "# Constants for dataset naming and category\n",
    "# CHANGE THESE BEFORE RUNNING\n",
    "CSV_NAME = 'CATEGORY_dataset.csv'\n",
    "PKL_NAME = 'Reddit_CATEGORY_original.pkl'\n",
    "CATEGORY = 'CATEGORY'  # Category for the dataset\n",
    "\n",
    "# Multiple subreddits to collect data from\n",
    "# CHANGE THESE BEFORE RUNNING\n",
    "subreddits = [\n",
    "    'subreddit1',\n",
    "    'subreddit2',\n",
    "]\n",
    "\n",
    "def is_moderator_or_bot_content(text, author_name=None):\n",
    "    \"\"\"Check if content is from moderators/bots or contains mod/bot language\"\"\"\n",
    "    \n",
    "    # Check for bot usernames\n",
    "    bot_usernames = [\n",
    "        'AutoModerator', 'auto-moderator', 'moderator', 'mod',\n",
    "        'bot', 'AutoBot', 'WikiTextBot', 'RepostSleuthBot',\n",
    "        'SnapshillBot', 'RemindMeBot', 'TweetPoster'\n",
    "    ]\n",
    "    \n",
    "    if author_name:\n",
    "        author_lower = author_name.lower()\n",
    "        for bot_name in bot_usernames:\n",
    "            if bot_name.lower() in author_lower:\n",
    "                return True\n",
    "    \n",
    "    # Common moderator/bot phrases (case insensitive)\n",
    "    mod_bot_phrases = [\n",
    "        r'this is a friendly reminder',\n",
    "        r'your post has been removed',\n",
    "        r'this comment has been removed',\n",
    "        r'are not allowed',\n",
    "        r'please read the rules',\n",
    "        r'violates rule',\n",
    "        r'breaking rule',\n",
    "        r'temporary ban',\n",
    "        r'permanently banned',\n",
    "        r'moderator action',\n",
    "        r'mod note',\n",
    "        r'subreddit rules',\n",
    "        r'community guidelines',\n",
    "        r'please contact the moderators',\n",
    "        r'message the mods',\n",
    "        r'if you have questions',\n",
    "        r'appeal this action',\n",
    "        r'repost will be removed',\n",
    "        r'spam filter',\n",
    "        r'automatically removed',\n",
    "        r'bot response',\n",
    "        r'i am a bot',\n",
    "        r'beep boop',\n",
    "        r'this action was performed automatically',\n",
    "        r'if you believe this was done in error',\n",
    "        r'contact.*moderator',\n",
    "        r'your submission.*removed',\n",
    "        r'thank you for your submission',\n",
    "        r'please ensure',\n",
    "        r'reminder.*rule',\n",
    "        r'this post.*locked',\n",
    "        r'comments.*locked'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for moderator/bot phrases\n",
    "    for phrase in mod_bot_phrases:\n",
    "        if re.search(phrase, text_lower):\n",
    "            return True\n",
    "    \n",
    "    # Check for overly formal/template language\n",
    "    template_patterns = [\n",
    "        r'^thank you for submitting',\n",
    "        r'^your post.*has been',\n",
    "        r'^this is an automated',\n",
    "        r'^please note that',\n",
    "        r'^as a reminder',\n",
    "        r'^unfortunately.*your'\n",
    "    ]\n",
    "    \n",
    "    for pattern in template_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def collect_subreddit_data(subreddit_name, limit=50):\n",
    "    \"\"\"Collect posts and top 5 comments from a specific subreddit\"\"\"\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts_data = []\n",
    "        \n",
    "        print(f\"Collecting from r/{subreddit_name}...\")\n",
    "        \n",
    "        # Iterate over the posts\n",
    "        for submission in subreddit.hot(limit=limit):  \n",
    "            if submission.stickied:  # Skip stickied posts\n",
    "                continue\n",
    "            \n",
    "            # Skip posts that look like moderator posts\n",
    "            if is_moderator_or_bot_content(submission.title + \" \" + submission.selftext, submission.author.name if submission.author else None):\n",
    "                continue\n",
    "                \n",
    "            submission.comments.replace_more(limit=0)  # Remove MoreComments instances\n",
    "            comments = submission.comments.list()\n",
    "            \n",
    "            # Filter out moderator/bot comments\n",
    "            filtered_comments = []\n",
    "            for comment in comments:\n",
    "                if hasattr(comment, 'body') and hasattr(comment, 'author'):\n",
    "                    author_name = comment.author.name if comment.author else None\n",
    "                    if not is_moderator_or_bot_content(comment.body, author_name):\n",
    "                        filtered_comments.append(comment)\n",
    "            \n",
    "            # Get top 5 comments based on score from filtered comments\n",
    "            top_comments = sorted(filtered_comments, key=lambda x: x.score, reverse=True)[:5]\n",
    "            comment_texts = [comment.body for comment in top_comments]\n",
    "            \n",
    "            # Ensure we have exactly 5 comments (pad with empty strings if needed)\n",
    "            while len(comment_texts) < 5:\n",
    "                comment_texts.append(\"\")\n",
    "            \n",
    "            # Create row for this post\n",
    "            post_data = {\n",
    "                'post_title': submission.title,\n",
    "                'post_body': submission.selftext,\n",
    "                'url': submission.url,\n",
    "                'top_5_comments': comment_texts[:5],\n",
    "                'subreddit': f\"r/{subreddit_name}\",\n",
    "                'category': CATEGORY,\n",
    "                'score': submission.score,\n",
    "                'num_comments': submission.num_comments\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "        print(f\"Collected {len(posts_data)} posts from r/{subreddit_name}\")\n",
    "        return posts_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting from r/{subreddit_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Collect data from all subreddits\n",
    "all_posts = []\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit_posts = collect_subreddit_data(subreddit_name, limit=50)\n",
    "    all_posts.extend(subreddit_posts)\n",
    "    \n",
    "    # Add small delay to be respectful to Reddit API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nTotal posts collected: {len(all_posts)}\")\n",
    "\n",
    "# Convert to DataFrame  \n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Create individual rows for posts and each of the top 5 comments\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Add row for the post itself\n",
    "    post_row = {\n",
    "        'text': f\"{row['post_title']} {row['post_body']}\".strip(),\n",
    "        'type': 'post',\n",
    "        'subreddit': row['subreddit'],\n",
    "        'category': row['category'],\n",
    "        'score': row['score'],\n",
    "        'url': row['url']\n",
    "    }\n",
    "    expanded_rows.append(post_row)\n",
    "    \n",
    "    # Add rows for each of the top 5 comments\n",
    "    for i, comment in enumerate(row['top_5_comments']):\n",
    "        if comment.strip():  # Only add non-empty comments\n",
    "            comment_row = {\n",
    "                'text': comment,\n",
    "                'type': f'comment_{i+1}',\n",
    "                'subreddit': row['subreddit'], \n",
    "                'category': row['category'],\n",
    "                'score': None,  # Comment scores not tracked individually here\n",
    "                'url': row['url']  # Link back to original post\n",
    "            }\n",
    "            expanded_rows.append(comment_row)\n",
    "\n",
    "# Create final dataset with individual rows\n",
    "final_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Breakdown by type: {final_df['type'].value_counts()}\")\n",
    "print(f\"Breakdown by subreddit: {final_df['subreddit'].value_counts()}\")\n",
    "\n",
    "# Save the dataset\n",
    "final_df.to_csv(CSV_NAME, index=False)\n",
    "print(f\"Dataset saved as '{CSV_NAME}'\")\n",
    "\n",
    "# Also save the original format for reference\n",
    "df.to_pickle(PKL_NAME)\n",
    "print(f\"Original format saved as '{PKL_NAME}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
