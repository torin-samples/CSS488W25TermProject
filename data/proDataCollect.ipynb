{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b59b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "#Use Flask to handle incoming requests from redirect URI from Reddit\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/reddit_callback')\n",
    "def reddit_callback():\n",
    "    # Retrieve the authorization code or access token from the URL parameters\n",
    "    authorization_code = request.args.get('code')\n",
    "    # Do something with the authorization code, such as exchanging it for an access token\n",
    "    # Or, store it for later use\n",
    "    return \"Callback received successfully\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b804c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting from r/CareerAdvice...\n",
      "Collected 49 posts from r/CareerAdvice\n",
      "Collecting from r/jobs...\n",
      "Collected 48 posts from r/jobs\n",
      "Collecting from r/resumes...\n",
      "Collected 48 posts from r/resumes\n",
      "Collecting from r/careerguidance...\n",
      "Collected 49 posts from r/careerguidance\n",
      "Collecting from r/cscareerquestions...\n",
      "Collected 48 posts from r/cscareerquestions\n",
      "Collecting from r/AskHR...\n",
      "Collected 49 posts from r/AskHR\n",
      "Collecting from r/recruitinghell...\n",
      "Collected 50 posts from r/recruitinghell\n",
      "Collecting from r/LinkedInLunatics...\n",
      "Collected 47 posts from r/LinkedInLunatics\n",
      "Collecting from r/interviews...\n",
      "Collected 49 posts from r/interviews\n",
      "Collecting from r/work...\n",
      "Collected 48 posts from r/work\n",
      "\n",
      "Total posts collected: 485\n",
      "Final dataset shape: (1957, 6)\n",
      "Breakdown by type: type\n",
      "post         485\n",
      "comment_1    378\n",
      "comment_2    325\n",
      "comment_3    278\n",
      "comment_4    259\n",
      "comment_5    232\n",
      "Name: count, dtype: int64\n",
      "Breakdown by subreddit: subreddit\n",
      "r/cscareerquestions    236\n",
      "r/recruitinghell       230\n",
      "r/LinkedInLunatics     230\n",
      "r/AskHR                225\n",
      "r/work                 209\n",
      "r/interviews           192\n",
      "r/jobs                 187\n",
      "r/careerguidance       176\n",
      "r/resumes              138\n",
      "r/CareerAdvice         134\n",
      "Name: count, dtype: int64\n",
      "Dataset saved as 'professional_dataset.csv'\n",
      "Original format saved as 'Reddit_professional_original.pkl'\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "    user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "    check_for_async=False\n",
    ")\n",
    "\n",
    "# List of professional/career subreddits\n",
    "professional_subreddits = [\n",
    "    'CareerAdvice',\n",
    "    'jobs',\n",
    "    'resumes',\n",
    "    'careerguidance',\n",
    "    'cscareerquestions',\n",
    "    'AskHR',\n",
    "    'recruitinghell',\n",
    "    'LinkedInLunatics',\n",
    "    'interviews',\n",
    "    'work'\n",
    "]\n",
    "\n",
    "def is_moderator_or_bot_content(text, author_name=None):\n",
    "    \"\"\"Filter out bot/moderator content.\"\"\"\n",
    "    bot_usernames = [\n",
    "        'AutoModerator', 'moderator', 'mod', 'bot', 'WikiTextBot', 'RepostSleuthBot',\n",
    "        'RemindMeBot', 'TweetPoster'\n",
    "    ]\n",
    "    if author_name:\n",
    "        author_lower = author_name.lower()\n",
    "        for bot_name in bot_usernames:\n",
    "            if bot_name.lower() in author_lower:\n",
    "                return True\n",
    "    mod_bot_phrases = [\n",
    "        r'this is a friendly reminder',\n",
    "        r'your post has been removed',\n",
    "        r'this comment has been removed',\n",
    "        r'are not allowed',\n",
    "        r'please read the rules',\n",
    "        r'violates rule',\n",
    "        r'breaking rule',\n",
    "        r'temporary ban',\n",
    "        r'permanently banned',\n",
    "        r'moderator action',\n",
    "        r'mod note',\n",
    "        r'subreddit rules',\n",
    "        r'community guidelines',\n",
    "        r'please contact the moderators',\n",
    "        r'message the mods',\n",
    "        r'if you have questions',\n",
    "        r'appeal this action',\n",
    "        r'repost will be removed',\n",
    "        r'spam filter',\n",
    "        r'automatically removed',\n",
    "        r'bot response',\n",
    "        r'i am a bot',\n",
    "        r'beep boop',\n",
    "        r'this action was performed automatically',\n",
    "        r'if you believe this was done in error',\n",
    "        r'contact.*moderator',\n",
    "        r'your submission.*removed',\n",
    "        r'thank you for your submission',\n",
    "        r'please ensure',\n",
    "        r'reminder.*rule',\n",
    "        r'this post.*locked',\n",
    "        r'comments.*locked'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for phrase in mod_bot_phrases:\n",
    "        if re.search(phrase, text_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def collect_subreddit_data(subreddit_name, limit=50):\n",
    "    \"\"\"Collect posts and top 5 comments from a specific subreddit.\"\"\"\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts_data = []\n",
    "        print(f\"Collecting from r/{subreddit_name}...\")\n",
    "        for submission in subreddit.hot(limit=limit):\n",
    "            if submission.stickied:\n",
    "                continue\n",
    "            if is_moderator_or_bot_content(submission.title + \" \" + submission.selftext, submission.author.name if submission.author else None):\n",
    "                continue\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            comments = submission.comments.list()\n",
    "            filtered_comments = []\n",
    "            for comment in comments:\n",
    "                if hasattr(comment, 'body') and hasattr(comment, 'author'):\n",
    "                    author_name = comment.author.name if comment.author else None\n",
    "                    if not is_moderator_or_bot_content(comment.body, author_name):\n",
    "                        filtered_comments.append(comment)\n",
    "            top_comments = sorted(filtered_comments, key=lambda x: x.score, reverse=True)[:5]\n",
    "            comment_texts = [comment.body for comment in top_comments]\n",
    "            while len(comment_texts) < 5:\n",
    "                comment_texts.append(\"\")\n",
    "            post_data = {\n",
    "                'post_title': submission.title,\n",
    "                'post_body': submission.selftext,\n",
    "                'url': submission.url,\n",
    "                'top_5_comments': comment_texts[:5],\n",
    "                'subreddit': f\"r/{subreddit_name}\",\n",
    "                'category': 'Professional',\n",
    "                'score': submission.score,\n",
    "                'num_comments': submission.num_comments\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "        print(f\"Collected {len(posts_data)} posts from r/{subreddit_name}\")\n",
    "        return posts_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting from r/{subreddit_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Collect data from all professional subreddits\n",
    "all_posts = []\n",
    "for subreddit_name in professional_subreddits:\n",
    "    subreddit_posts = collect_subreddit_data(subreddit_name, limit=50)\n",
    "    all_posts.extend(subreddit_posts)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nTotal posts collected: {len(all_posts)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Expand posts and comments into individual rows\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    post_row = {\n",
    "        'text': f\"{row['post_title']} {row['post_body']}\".strip(),\n",
    "        'type': 'post',\n",
    "        'subreddit': row['subreddit'],\n",
    "        'category': row['category'],\n",
    "        'score': row['score'],\n",
    "        'url': row['url']\n",
    "    }\n",
    "    expanded_rows.append(post_row)\n",
    "    for i, comment in enumerate(row['top_5_comments']):\n",
    "        if comment.strip():\n",
    "            comment_row = {\n",
    "                'text': comment,\n",
    "                'type': f'comment_{i+1}',\n",
    "                'subreddit': row['subreddit'],\n",
    "                'category': row['category'],\n",
    "                'score': None,\n",
    "                'url': row['url']\n",
    "            }\n",
    "            expanded_rows.append(comment_row)\n",
    "\n",
    "final_df = pd.DataFrame(expanded_rows)\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Breakdown by type: {final_df['type'].value_counts()}\")\n",
    "print(f\"Breakdown by subreddit: {final_df['subreddit'].value_counts()}\")\n",
    "\n",
    "# Save the dataset\n",
    "final_df.to_csv('professional_dataset.csv', index=False)\n",
    "print(\"Dataset saved as 'professional_dataset.csv'\")\n",
    "\n",
    "df.to_pickle('Reddit_professional_original.pkl')\n",
    "print(\"Original format saved as 'Reddit_professional_original.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
